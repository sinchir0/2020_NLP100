{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "問題 : https://nlp100.github.io/ja/ch01.html<br>\n",
    "wakameさん回答 : https://github.com/wakamezake/nlp_q100_2020<br>\n",
    "u++さん回答 : https://github.com/upura/nlp100v2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第1章: 準備運動\n",
    "## 00. 文字列の逆順\n",
    "文字列”stressed”の文字を逆に（末尾から先頭に向かって）並べた文字列を得よ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'stressed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ''\n",
    "\n",
    "for i in range(1,len(text)+1):\n",
    "    s = s + text[-i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desserts\n"
     ]
    }
   ],
   "source": [
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wakameさんの回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desserts\n"
     ]
    }
   ],
   "source": [
    "q = \"stressed\"\n",
    "print(q[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### u++さんの回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desserts\n"
     ]
    }
   ],
   "source": [
    "text = 'stressed'\n",
    "print(text[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. 「パタトクカシーー」\n",
    "「パタトクカシーー」という文字列の1,3,5,7文字目を取り出して連結した文字列を得よ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'タクシー'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = 'パタトクカシーー'\n",
    "input[1::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wakameさんの回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "タクシー\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "https://nlp100.github.io/ja/ch01.html#01-%E3%83%91%E3%82%BF%E3%83%88%E3%82%AF%E3%82%AB%E3%82%B7%E3%83%BC%E3%83%BC\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    q = \"パタトクカシーー\"\n",
    "    print(q[1::2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### u++さんの回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "タクシー\n"
     ]
    }
   ],
   "source": [
    "text = 'パタトクカシーー'\n",
    "print(text[1::2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. 「パトカー」＋「タクシー」＝「パタトクカシーー」\n",
    "「パトカー」＋「タクシー」の文字を先頭から交互に連結して文字列「パタトクカシーー」を得よ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = 'パトカー'\n",
    "text2 = 'タクシー'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = ''\n",
    "\n",
    "for i in range(4):\n",
    "    ans += text1[i] + text2[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "パタトクカシーー\n"
     ]
    }
   ],
   "source": [
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wakameさんの回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "パタトクカシーー\n"
     ]
    }
   ],
   "source": [
    "q1 = \"パトカー\"\n",
    "q2 = \"タクシー\"\n",
    "q = \"\"\n",
    "for a, b in zip(q1, q2):\n",
    "    q += a + b\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### u++さんの回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "パタトクカシーー\n"
     ]
    }
   ],
   "source": [
    "text0 = 'パトカー'\n",
    "text1 = 'タクシー'\n",
    "ans = ''\n",
    "\n",
    "for i in range(len(text0)):\n",
    "    ans += text0[i]\n",
    "    ans += text1[i]\n",
    "\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. 円周率\n",
    "“Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.”という文を単語に分解し，各単語の（アルファベットの）文字数を先頭から出現順に並べたリストを作成せよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = re.compile(\"[^a-zA-Z]\")\n",
    "\n",
    "len_list = [(len(pat.sub(\"\",text_element))) for text_element in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8, 9, 7, 9]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wakameさんの回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8, 9, 7, 9]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "q = \"Now I need a drink, alcoholic of course, after the heavy lectures\" \\\n",
    "    \" involving quantum mechanics.\"\n",
    "# skip [.,]\n",
    "pat = re.compile(\"[^a-zA-Z]\")\n",
    "print([len(pat.sub(\"\", word)) for word in q.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### u++さんの回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8, 9, 7, 9]\n"
     ]
    }
   ],
   "source": [
    "rawText = 'Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.'\n",
    "text = rawText.replace('.', '').replace(',', '')\n",
    "ans = [len(w) for w in text.split()]\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. 元素記号\n",
    "“Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.”という文を単語に分解し，1, 5, 6, 7, 8, 9, 15, 16, 19番目の単語は先頭の1文字，それ以外の単語は先頭に2文字を取り出し，取り出した文字列から単語の位置（先頭から何番目の単語か）への連想配列（辞書型もしくはマップ型）を作成せよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawText = \"Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.\"\n",
    "text = rawText.replace('.', '').replace(',', '')\n",
    "word_list = [w for w in text.split()]\n",
    "number_extract1 = [1, 5, 6, 7, 8, 9, 15, 16, 19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_extract_list = []\n",
    "word_position_list = []\n",
    "\n",
    "for i,word in enumerate(word_list):\n",
    "    word_position_list.append(i+1)\n",
    "    if (i + 1) in number_extract1:\n",
    "        word_extract_list.append(word[0])\n",
    "    else:\n",
    "        word_extract_list.append(word[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'H': 1, 'He': 2, 'Li': 3, 'Be': 4, 'B': 5, 'C': 6, 'N': 7, 'O': 8, 'F': 9, 'Ne': 10, 'Na': 11, 'Mi': 12, 'Al': 13, 'Si': 14, 'P': 15, 'S': 16, 'Cl': 17, 'Ar': 18, 'K': 19, 'Ca': 20}\n"
     ]
    }
   ],
   "source": [
    "ans_dict = dict(zip(word_extract_list,word_position_list))\n",
    "print(ans_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wakameさんの回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'H': 1, 'He': 2, 'Li': 3, 'Be': 4, 'B': 5, 'C': 6, 'N': 7, 'O': 8, 'F': 9, 'Ne': 10, 'Na': 11, 'Mi': 12, 'Al': 13, 'Si': 14, 'P': 15, 'S': 16, 'Cl': 17, 'Ar': 18, 'K': 19, 'Ca': 20}\n"
     ]
    }
   ],
   "source": [
    "q = \"Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations\" \\\n",
    "    \" Might Also Sign Peace Security Clause. Arthur King Can.\"\n",
    "index = [1, 5, 6, 7, 8, 9, 15, 16, 19]\n",
    "result = {}\n",
    "words = q.split()\n",
    "for idx, word in enumerate(words, 1):\n",
    "    if idx in index:\n",
    "        result[word[0]] = idx\n",
    "    else:\n",
    "        result[word[:2]] = idx\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### u++さんの回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'H': 1, 'He': 2, 'Li': 3, 'Be': 4, 'B': 5, 'C': 6, 'N': 7, 'O': 8, 'F': 9, 'Ne': 10, 'Na': 11, 'Mi': 12, 'Al': 13, 'Si': 14, 'P': 15, 'S': 16, 'Cl': 17, 'Ar': 18, 'K': 19, 'Ca': 20}\n"
     ]
    }
   ],
   "source": [
    "def extWord(i, word):\n",
    "    if i in [1, 5, 6, 7, 8, 9, 15, 16, 19]:\n",
    "        return (word[0], i)\n",
    "    else:\n",
    "        return (word[:2], i)\n",
    "\n",
    "\n",
    "rawText = 'Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.'\n",
    "text = rawText.replace('.', '').replace(',', '')\n",
    "ans = [extWord(i, w) for i, w in enumerate(text.split(), 1)]\n",
    "print(dict(ans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. n-gram\n",
    "与えられたシーケンス（文字列やリストなど）からn-gramを作る関数を作成せよ．この関数を用い，”I am an NLPer”という文から単語bi-gram，文字bi-gramを得よ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawText = 'I am an NLPer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref : https://qiita.com/kazmaw/items/4df328cba6429ec210fb\n",
    "class ngram():\n",
    "    def __init__(self,rawText):\n",
    "        self.rawtext = rawText\n",
    "        \n",
    "    def get_word_ngram(self,n):\n",
    "        return [ self.rawtext[idx:idx + n] for idx in range(len(self.rawtext) - n + 1)]\n",
    "    \n",
    "    def get_char_ngram(self,n):\n",
    "        text = self.rawtext.split()\n",
    "        return [ text[idx:idx + n] for idx in range(len(text) - n + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I ', ' a', 'am', 'm ', ' a', 'an', 'n ', ' N', 'NL', 'LP', 'Pe', 'er']\n",
      "[['I', 'am'], ['am', 'an'], ['an', 'NLPer']]\n"
     ]
    }
   ],
   "source": [
    "ngram = ngram(rawText)\n",
    "print(ngram.get_word_ngram(n=2))\n",
    "print(ngram.get_char_ngram(n=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wakameさんの回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', 'a', 'm', ' ', 'a', 'n', ' ', 'N', 'L', 'P', 'e', 'r']\n",
      "['I ', ' a', 'am', 'm ', ' a', 'an', 'n ', ' N', 'NL', 'LP', 'Pe', 'er']\n",
      "['I a', ' am', 'am ', 'm a', ' an', 'an ', 'n N', ' NL', 'NLP', 'LPe', 'Per']\n",
      "[['I'], ['am'], ['an'], ['NLPer']]\n",
      "[['I', 'am'], ['am', 'an'], ['an', 'NLPer']]\n",
      "[['I', 'am', 'an'], ['am', 'an', 'NLPer']]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "https://nlp100.github.io/ja/ch01.html#05-n-gram\n",
    "\"\"\"\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "def n_gram(_words: Union[str, list], n_split: int):\n",
    "    for i in range(len(_words) - n_split + 1):\n",
    "        yield _words[i:i + n_split]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    q = \"I am an NLPer\"\n",
    "    # gram\n",
    "    print(list(n_gram(q, 1)))\n",
    "    print(list(n_gram(q, 2)))\n",
    "    print(list(n_gram(q, 3)))\n",
    "    # word gram\n",
    "    words = q.split()\n",
    "    print(list(n_gram(words, 1)))\n",
    "    print(list(n_gram(words, 2)))\n",
    "    print(list(n_gram(words, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', 'a', 'm', ' ', 'a', 'n', ' ', 'N', 'L', 'P', 'e', 'r']\n",
      "[['I'], ['am'], ['an'], ['NLPer']]\n",
      "['I ', ' a', 'am', 'm ', ' a', 'an', 'n ', ' N', 'NL', 'LP', 'Pe', 'er']\n",
      "[['I', 'am'], ['am', 'an'], ['an', 'NLPer']]\n",
      "['I a', ' am', 'am ', 'm a', ' an', 'an ', 'n N', ' NL', 'NLP', 'LPe', 'Per']\n",
      "[['I', 'am', 'an'], ['am', 'an', 'NLPer']]\n"
     ]
    }
   ],
   "source": [
    "def n_gram(target, n):\n",
    "    return [target[idx:idx + n] for idx in range(len(target) - n + 1)]\n",
    "\n",
    "\n",
    "text = 'I am an NLPer'\n",
    "for i in range(1, 4):\n",
    "    print(n_gram(text, i))\n",
    "    print(n_gram(text.split(' '), i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06. 集合\n",
    "“paraparaparadise”と”paragraph”に含まれる文字bi-gramの集合を，それぞれ, XとYとして求め，XとYの和集合，積集合，差集合を求めよ．さらに，’se’というbi-gramがXおよびYに含まれるかどうかを調べよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawText_1 = 'paraparaparadise'\n",
    "rawText_2 = 'paragraph'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref : https://qiita.com/kazmaw/items/4df328cba6429ec210fb\n",
    "class ngram():\n",
    "    def __init__(self,rawText):\n",
    "        self.rawtext = rawText\n",
    "        \n",
    "    def get_word_ngram(self,n):\n",
    "        return [ self.rawtext[idx:idx + n] for idx in range(len(self.rawtext) - n + 1)]\n",
    "    \n",
    "    def get_char_ngram(self,n):\n",
    "        text = self.rawtext.split()\n",
    "        return [ text[idx:idx + n] for idx in range(len(text) - n + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:['pa', 'ar', 'ra', 'ap', 'pa', 'ar', 'ra', 'ap', 'pa', 'ar', 'ra', 'ad', 'di', 'is', 'se']\n",
      "Y:['pa', 'ar', 'ra', 'ag', 'gr', 'ra', 'ap', 'ph']\n"
     ]
    }
   ],
   "source": [
    "X = ngram(rawText_1).get_word_ngram(2)\n",
    "Y = ngram(rawText_2).get_word_ngram(2)\n",
    "print(f'X:{X}')\n",
    "print(f'Y:{Y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "和集合\n",
      "{'gr', 'ag', 'ra', 'pa', 'is', 'se', 'ad', 'di', 'ph', 'ap', 'ar'}\n",
      "積集合\n",
      "{'pa', 'ar', 'ap', 'ra'}\n",
      "差集合\n",
      "{'di', 'se', 'ad', 'is'}\n"
     ]
    }
   ],
   "source": [
    "print('和集合')\n",
    "print(set(X) | set(Y))\n",
    "\n",
    "print('積集合')\n",
    "print(set(X) & set(Y))\n",
    "\n",
    "print('差集合')\n",
    "print(set(X) - set(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print('se' in X)\n",
    "print('se' in Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wakameさんの回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gr', 'ag', 'ra', 'pa', 'is', 'se', 'ad', 'di', 'ph', 'ap', 'ar'}\n",
      "{'pa', 'ar', 'ap', 'ra'}\n",
      "{'di', 'se', 'ad', 'is'}\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "  \"\"\"\n",
    "https://nlp100.github.io/ja/ch01.html#06-%E9%9B%86%E5%90%88\n",
    "\"\"\"\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "def n_gram(_words: Union[str, list], n_split: int):\n",
    "    for i in range(len(_words) - n_split + 1):\n",
    "        yield _words[i:i + n_split]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    q1 = \"paraparaparadise\"\n",
    "    q2 = \"paragraph\"\n",
    "\n",
    "    w1 = set(list(n_gram(q1, 2)))\n",
    "    w2 = set(list(n_gram(q2, 2)))\n",
    "\n",
    "    print(w1.union(w2))\n",
    "    print(w1.intersection(w2))\n",
    "    print(w1.difference(w2))\n",
    "\n",
    "    q = \"se\"\n",
    "\n",
    "    print(q in w1)\n",
    "    print(q in w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### u++さんの回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "和集合: {'gr', 'ag', 'ra', 'pa', 'is', 'se', 'ad', 'di', 'ph', 'ap', 'ar'}\n",
      "積集合: {'pa', 'ar', 'ap', 'ra'}\n",
      "差集合: {'di', 'se', 'ad', 'is'}\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "def n_gram(target, n):\n",
    "    return [target[idx:idx + n] for idx in range(len(target) - n + 1)]\n",
    "\n",
    "\n",
    "X_text = 'paraparaparadise'\n",
    "Y_text = 'paragraph'\n",
    "X = n_gram(X_text, 2)\n",
    "Y = n_gram(Y_text, 2)\n",
    "\n",
    "print(f'和集合: {set(X) | set(Y)}')\n",
    "print(f'積集合: {set(X) & set(Y)}')\n",
    "print(f'差集合: {set(X) - set(Y)}')\n",
    "print('se' in (set(X) & set(Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07. テンプレートによる文生成\n",
    "引数x, y, zを受け取り「x時のyはz」という文字列を返す関数を実装せよ．さらに，x=12, y=”気温”, z=22.4として，実行結果を確認せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sentence_template(x,y,z):\n",
    "    return f'{x}時の{y}は{z}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12時の気温は22.4'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=12; y='気温'; z=22.4\n",
    "make_sentence_template(x,y,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wakameさんの回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12時の気温は22.4\n"
     ]
    }
   ],
   "source": [
    "def template(x, y, z):\n",
    "    return \"{}時の{}は{}\".format(x, y, z)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(template(x=12, y=\"気温\", z=22.4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### u++さんの回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12時の気温は22.4\n"
     ]
    }
   ],
   "source": [
    "  def genText(x, y, z):\n",
    "    return f'{x}時の{y}は{z}'\n",
    "\n",
    "\n",
    "x = 12\n",
    "y = '気温'\n",
    "z = 22.4\n",
    "print(genText(x, y, z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08. 暗号文\n",
    "与えられた文字列の各文字を，以下の仕様で変換する関数cipherを実装せよ．\n",
    "\n",
    "英小文字ならば(219 - 文字コード)の文字に置換\n",
    "その他の文字はそのまま出力\n",
    "この関数を用い，英語のメッセージを暗号化・復号化せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akkov\n",
      "Apple\n"
     ]
    }
   ],
   "source": [
    "def encrypt_decrypt(rawText):\n",
    "\n",
    "    # u++さん回答参考に再実装\n",
    "    cipher_word = [chr(219 - ord(word)) if word.islower() else word for word in rawText]\n",
    "    return ''.join(cipher_word)\n",
    "\n",
    "# encrypt\n",
    "print(encrypt_decrypt(\"Apple\"))\n",
    "# decrypt\n",
    "print(encrypt_decrypt(\"Akkov\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wakameさんの回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I zn zm NLPvi\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "https://nlp100.github.io/ja/ch01.html#08-%E6%9A%97%E5%8F%B7%E6%96%87\n",
    "\"\"\"\n",
    "import re\n",
    "\n",
    "\n",
    "def cipher(word):\n",
    "    pat = re.compile(\"[a-z]\")\n",
    "    result = \"\"\n",
    "    for w in word:\n",
    "        match = pat.search(w)\n",
    "        if match:\n",
    "            result += chr(219 - ord(w))\n",
    "        else:\n",
    "            result += w\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    q = \"I am an NLPer\"\n",
    "    print(cipher(q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### u++さんの回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gsrh rh z nvhhztv.\n",
      "this is a message.\n"
     ]
    }
   ],
   "source": [
    "def cipher(text):\n",
    "    text = [chr(219 - ord(w)) if 97 <= ord(w) <= 122 else w for w in text]\n",
    "    return ''.join(text)\n",
    "\n",
    "\n",
    "text = 'this is a message.'\n",
    "ans = cipher(text)\n",
    "print(ans)\n",
    "ans = cipher(ans)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09. Typoglycemia\n",
    "スペースで区切られた単語列に対して，各単語の先頭と末尾の文字は残し，それ以外の文字の順序をランダムに並び替えるプログラムを作成せよ．ただし，長さが４以下の単語は並び替えないこととする．適当な英語の文（例えば”I couldn’t believe that I could actually understand what I was reading : the phenomenal power of the human mind .”）を与え，その実行結果を確認せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawText = 'I couldn’t believe that I could actually understand what I was reading : the phenomenal power of the human mind .'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I cndolu’t beviele that I colud atualcly utresnandd what I was reanidg : the phnnmeeaol power of the hmuan mind .'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "ResultText = []\n",
    "\n",
    "for word in rawText.split():\n",
    "    if len(word) <= 4:\n",
    "        ResultText.append(word)\n",
    "    else:\n",
    "        word_contents_random = ''.join(random.sample(word[1:-1], len(word[1:-1])))\n",
    "        ResultText.append(word[0] + word_contents_random + word[-1])\n",
    "        \n",
    "' '.join(ResultText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wakameさんの回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Icdol’nutbevlieethatIcuoldatualclyuandsntredwhatIwasraidneg:thepnmenohealpoewrofthehmuanmind.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "https://nlp100.github.io/ja/ch01.html#09-typoglycemia\n",
    "\"\"\"\n",
    "import random\n",
    "\n",
    "\n",
    "def f(_word):\n",
    "    if len(_word) > 4:\n",
    "        rand_seq = _word[1:-1]\n",
    "        rand_words = \"\".join(random.sample(rand_seq, len(rand_seq)))\n",
    "        return _word[0] + rand_words + _word[-1]\n",
    "    return _word\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    q = \"I couldn’t believe that I could actually understand what I was\" \\\n",
    "        \" reading : the phenomenal power of the human mind .\"\n",
    "    print(\"\".join(f(word) for word in q.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# u++さんの回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'conldu’t', 'bleivee', 'that', 'I', 'cuold', 'aaltucly', 'unersndtad', 'what', 'I', 'was', 'reinadg', ':', 'the', 'poaenhemnl', 'poewr', 'of', 'the', 'hmuan', 'mind', '.']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def shuffleWord(word):\n",
    "    if len(word) <= 4:\n",
    "        return word\n",
    "    else:\n",
    "        start = word[0]\n",
    "        end = word[-1]\n",
    "        others = random.sample(list(word[1:-1]), len(word[1:-1]))\n",
    "        return ''.join([start] + others + [end])\n",
    "\n",
    "\n",
    "text = 'I couldn’t believe that I could actually understand what I was reading : the phenomenal power of the human mind .'\n",
    "ans = [shuffleWord(w) for w in text.split()]\n",
    "print(ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
